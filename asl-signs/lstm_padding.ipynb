{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d1d5478",
   "metadata": {},
   "source": [
    "[Article](https://www.dotlayer.org/en/training-rnn-using-pytorch/)\n",
    "\n",
    "[Stanford RNN Reference](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "429d13c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "import gzip\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import shutil\n",
    "import sys\n",
    "import warnings\n",
    "from io import TextIOBase\n",
    "\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "import requests\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.functional import cross_entropy\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "data_wd = Path.home() / '.data' / 'asl-signs' / 'sequence_padding'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdd007f",
   "metadata": {},
   "source": [
    "# The Pytorch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "74ca97e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension = 300\n",
    "num_layer = 1\n",
    "bidirectional = False\n",
    "\n",
    "lstm_network = nn.LSTM(\n",
    "    input_size=dimension,\n",
    "    hidden_size=dimension,\n",
    "    num_layers=num_layer,\n",
    "    bidirectional=bidirectional,\n",
    "    batch_first=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2167db8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = dimension  # the output of the LSTM\n",
    "tag_dimension = 8\n",
    "\n",
    "fully_connected_network = nn.Linear(input_dim, tag_dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f210eae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "batch_size = 128\n",
    "lr = 0.1\n",
    "\n",
    "epoch_number = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e147a8a7",
   "metadata": {},
   "source": [
    "# The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "76afbdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_data(data_type):\n",
    "    \"\"\"\n",
    "    Function to download the dataset using data_type to specify if we want the train, valid or test.\n",
    "    \"\"\"\n",
    "\n",
    "    # hardcoded url to download the pickled dataset\n",
    "    root_url = \"https://dot-layer.github.io/blog-external-assets/train_rnn/{}.p\"\n",
    "\n",
    "    url = root_url.format(data_type)\n",
    "    r = requests.get(url)\n",
    "\n",
    "    with open(data_wd / f\"{data_type}.p\", \"wb\") as f:\n",
    "        f.write(r.content)\n",
    "\n",
    "\n",
    "# download_data(\"train\")\n",
    "# download_data(\"valid\")\n",
    "# download_data(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d042a4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import builtins\n",
    "import io\n",
    "import pickle\n",
    "\n",
    "safe_builtins = {\n",
    "    'set',\n",
    "    'list',\n",
    "    'tuple'\n",
    "}\n",
    "\n",
    "class RestrictedUnpickler(pickle.Unpickler):\n",
    "\n",
    "    def find_class(self, module, name):\n",
    "        # Only allow safe classes from builtins.\n",
    "        if module == \"builtins\" and name in safe_builtins:\n",
    "            return getattr(builtins, name)\n",
    "        # Forbid everything else.\n",
    "        raise pickle.UnpicklingError(\"global '%s.%s' is forbidden\" %\n",
    "                                     (module, name))\n",
    "\n",
    "def restricted_loads(s):\n",
    "    \"\"\"Helper function analogous to pickle.loads().\"\"\"\n",
    "    return RestrictedUnpickler(io.BytesIO(s)).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "70304a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(data_wd / 'train.p', mode='rb') as f:\n",
    "#     train_data = restricted_loads(f.read())\n",
    "    \n",
    "# with open(data_wd / 'valid.p', mode='rb') as f:\n",
    "#     valid_data = restricted_loads(f.read())\n",
    "    \n",
    "# with open(data_wd / 'test.p', mode='rb') as f:\n",
    "#     test_data = restricted_loads(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e2c2e499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(data_wd / 'train.json', 'w') as f:\n",
    "#     json.dump(train_data, f)\n",
    "    \n",
    "# with open(data_wd / 'valid.json', 'w') as f:\n",
    "#     json.dump(valid_data, f)\n",
    "    \n",
    "# with open(data_wd / 'test.json', 'w') as f:\n",
    "#     json.dump(test_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5533348c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_wd / 'train.json') as f:\n",
    "    train_data = json.load(f)\n",
    "    \n",
    "with open(data_wd / 'valid.json') as f:\n",
    "    valid_data = json.load(f)\n",
    "    \n",
    "with open(data_wd / 'test.json') as f:\n",
    "    test_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "99b5ce66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(728789, 182198, 100000)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(valid_data), len(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d8a7b0",
   "metadata": {},
   "source": [
    "# Vectorize the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6573a032",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LookForProgress(TextIOBase):\n",
    "    def __init__(self, stdout):\n",
    "        self.stdout = stdout\n",
    "        self.regex = re.compile(r\"([0-9]+(\\.[0-9]+)?%)\", re.IGNORECASE)\n",
    "\n",
    "    def write(self, o):\n",
    "        res = self.regex.findall(o)\n",
    "        if len(res) != 0:\n",
    "            print(f\"\\r{res[-1][0]}\", end=\"\", file=self.stdout)\n",
    "            \n",
    "            \n",
    "class EmbeddingVectorizer:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Embedding vectorizer\n",
    "        \"\"\"\n",
    "        with contextlib.redirect_stdout(LookForProgress(sys.stdout)):\n",
    "            # We use a context manager redirect to handle the broken download in\n",
    "            # Jupyter notebook\n",
    "            fasttext.util.download_model(\"fr\", if_exists=\"ignore\")\n",
    "        self.embedding_model = fasttext.load_model(\"./cc.fr.300.bin\")\n",
    "\n",
    "    def __call__(self, address):\n",
    "        \"\"\"\n",
    "        Convert address to embedding vectors\n",
    "        :param address: The address to convert\n",
    "        :return: The embeddings vectors\n",
    "        \"\"\"\n",
    "        embeddings = []\n",
    "        for word in address.split():\n",
    "            embeddings.append(self.embedding_model[word])\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6d08000d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "embedding_vectorizer = EmbeddingVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "26f32880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('35 r de perc√© gatineau qc j8r 2e6', 8, (300,))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "addr_record = train_data[0][0]\n",
    "addr_vector = embedding_vectorizer(addr_record)\n",
    "addr_record, len(addr_vector), addr_vector[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "afaa6857",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetBucket:\n",
    "    def __init__(self, data, embedding_vectorizer):\n",
    "        self.data = data\n",
    "        self.embedding_vectorizer = embedding_vectorizer\n",
    "        self.tags_set = {\n",
    "            \"StreetNumber\": 0,\n",
    "            \"StreetName\": 1,\n",
    "            \"Unit\": 2,\n",
    "            \"Municipality\": 3,\n",
    "            \"Province\": 4,\n",
    "            \"PostalCode\": 5,\n",
    "            \"Orientation\": 6,\n",
    "            \"GeneralDelivery\": 7,\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, item):  # We vectorize when data is asked\n",
    "        data = self.data[item]\n",
    "        return self._item_vectorizing(data)\n",
    "\n",
    "    def _item_vectorizing(self, item):\n",
    "        address = item[0]\n",
    "        address_vector = self.embedding_vectorizer(address)\n",
    "\n",
    "        tags = item[1]\n",
    "        idx_tags = self._convert_tags_to_idx(tags)\n",
    "\n",
    "        return address_vector, idx_tags\n",
    "\n",
    "    def _convert_tags_to_idx(self, tags):\n",
    "        idx_tags = []\n",
    "        for tag in tags:\n",
    "            idx_tags.append(self.tags_set[tag])\n",
    "        return idx_tags\n",
    "    \n",
    "train_dataset_vectorizer = DatasetBucket(train_data, embedding_vectorizer)\n",
    "valid_dataset_vectorizer = DatasetBucket(valid_data, embedding_vectorizer)\n",
    "test_dataset_vectorizer = DatasetBucket(test_data, embedding_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0664127d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 1, 1, 3, 4, 5, 5]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "address, tag = train_dataset_vectorizer[0]\n",
    "tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "523e9d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    The collate_fn that can add padding to the sequences so all can have\n",
    "    the same length as the longest one.\n",
    "\n",
    "    Args:\n",
    "        batch (List[List, List]): The batch data, where the first element\n",
    "        of the tuple is the word idx and the second element are the target\n",
    "        label.\n",
    "\n",
    "    Returns:\n",
    "        A tuple (x, y). The element x is a tensor of packed sequence .\n",
    "        The element y is a tensor of padded tag indices. The word vectors are\n",
    "        padded with vectors of 0s and the tag indices are padded with -100s.\n",
    "        Padding with -100 is done because of the cross-entropy loss and the\n",
    "        accuracy metric ignores the targets with values -100.\n",
    "    \"\"\"\n",
    "\n",
    "    # This gets us two lists of tensors and a list of integer.\n",
    "    # Each tensor in the first list is a sequence of word vectors.\n",
    "    # Each tensor in the second list is a sequence of tag indices.\n",
    "    # The list of integer consist of the lengths of the sequences in order.\n",
    "    sequences_vectors, sequences_labels, lengths = zip(\n",
    "        *[\n",
    "            (torch.FloatTensor(seq_vectors), torch.LongTensor(labels), len(seq_vectors))\n",
    "            for (seq_vectors, labels) in sorted(\n",
    "                batch, key=lambda x: len(x[0]), reverse=True\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    lengths = torch.LongTensor(lengths)\n",
    "\n",
    "    padded_sequences_vectors = pad_sequence(\n",
    "        sequences_vectors, batch_first=True, padding_value=0\n",
    "    )\n",
    "    pack_padded_sequences_vectors = pack_padded_sequence(\n",
    "        padded_sequences_vectors, lengths.cpu(), batch_first=True\n",
    "    )  # We pack the padded sequence to improve the computational speed during training\n",
    "\n",
    "    padded_sequences_labels = pad_sequence(\n",
    "        sequences_labels, batch_first=True, padding_value=-100\n",
    "    )\n",
    "\n",
    "    return pack_padded_sequences_vectors, padded_sequences_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fc01eea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset_vectorizer,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=pad_collate_fn,\n",
    "    num_workers=4,\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset_vectorizer,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=pad_collate_fn,\n",
    "    num_workers=4,\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset_vectorizer,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=pad_collate_fn,\n",
    "    num_workers=2,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5fb339ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecurrentNet(nn.Module):\n",
    "    def __init__(self, lstm_network, fully_connected_network):\n",
    "        super().__init__()\n",
    "        self.hidden_state = None\n",
    "\n",
    "        self.lstm_network = lstm_network\n",
    "        self.fully_connected_network = fully_connected_network\n",
    "\n",
    "    def forward(self, packed_sequences_vectors):\n",
    "        \"\"\"\n",
    "        Defines the computation performed at every call.\n",
    "\n",
    "        Shapes:\n",
    "            packed_sequence_vectors: batch_size * longest_sequence_length (padding), 300\n",
    "\n",
    "        \"\"\"\n",
    "        lstm_out, self.hidden_state = self.lstm_network(packed_sequences_vectors)\n",
    "        lstm_out, _ = pad_packed_sequence(lstm_out, batch_first=True)\n",
    "\n",
    "        tag_space = self.fully_connected_network(lstm_out)\n",
    "        return tag_space.transpose(-1, 1)  # We need to transpose since it's a sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "efa3b9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RecurrentNet(lstm_network, fully_connected_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e51486e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7831/1983819031.py:25: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484806139/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  (torch.FloatTensor(seq_vectors), torch.LongTensor(labels), len(seq_vectors))\n",
      "/tmp/ipykernel_7831/1983819031.py:25: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484806139/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  (torch.FloatTensor(seq_vectors), torch.LongTensor(labels), len(seq_vectors))\n",
      "/tmp/ipykernel_7831/1983819031.py:25: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484806139/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  (torch.FloatTensor(seq_vectors), torch.LongTensor(labels), len(seq_vectors))\n",
      "/tmp/ipykernel_7831/1983819031.py:25: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484806139/work/torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  (torch.FloatTensor(seq_vectors), torch.LongTensor(labels), len(seq_vectors))\n"
     ]
    }
   ],
   "source": [
    "addr_vectors, label_vectors = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3090e3cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[-0.0020, -0.0307, -0.0408,  ..., -0.0066, -0.0041, -0.0457],\n",
       "          [ 0.0245, -0.0144, -0.0243,  ..., -0.0119, -0.0015, -0.0477],\n",
       "          [ 0.0674, -0.0033, -0.0151,  ..., -0.0259,  0.0529, -0.0423],\n",
       "          ...,\n",
       "          [-0.0216, -0.1179,  0.0279,  ..., -0.0391, -0.0184, -0.0488],\n",
       "          [-0.0084, -0.0757,  0.0034,  ..., -0.0176, -0.0109, -0.0361],\n",
       "          [ 0.0021, -0.0654,  0.0006,  ..., -0.0034,  0.0037, -0.0398]],\n",
       " \n",
       "         [[-0.0014, -0.0335, -0.0325,  ..., -0.0015, -0.0128, -0.0471],\n",
       "          [ 0.0225, -0.0139, -0.0179,  ..., -0.0089, -0.0057, -0.0476],\n",
       "          [ 0.0650, -0.0021, -0.0111,  ..., -0.0234,  0.0508, -0.0415],\n",
       "          ...,\n",
       "          [-0.0278, -0.1195,  0.0308,  ..., -0.0364, -0.0188, -0.0564],\n",
       "          [-0.0096, -0.0758,  0.0049,  ..., -0.0116, -0.0102, -0.0396],\n",
       "          [-0.0047, -0.0668, -0.0145,  ..., -0.0143,  0.0039, -0.0536]],\n",
       " \n",
       "         [[-0.0136, -0.0182, -0.0153,  ..., -0.0119,  0.0219,  0.0002],\n",
       "          [-0.0013, -0.0186, -0.0174,  ..., -0.0092,  0.0236, -0.0119],\n",
       "          [-0.0055, -0.0148, -0.0189,  ..., -0.0060,  0.0129, -0.0110],\n",
       "          ...,\n",
       "          [ 0.0163, -0.0308, -0.0183,  ...,  0.0214,  0.0114, -0.0136],\n",
       "          [ 0.0207, -0.0300, -0.0167,  ...,  0.0173,  0.0147, -0.0091],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "         ...,\n",
       " \n",
       "         [[-0.0089,  0.0015, -0.0388,  ..., -0.0108,  0.0040, -0.0137],\n",
       "          [-0.0003, -0.0082, -0.0278,  ...,  0.0112,  0.0182, -0.0120],\n",
       "          [ 0.0027, -0.0136, -0.0252,  ...,  0.0096,  0.0112, -0.0151],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "         [[-0.0042, -0.0255, -0.0358,  ..., -0.0073,  0.0031, -0.0056],\n",
       "          [-0.0066, -0.0141, -0.0270,  ...,  0.0106,  0.0131, -0.0037],\n",
       "          [ 0.0026, -0.0165, -0.0201,  ...,  0.0071,  0.0088, -0.0054],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       " \n",
       "         [[ 0.0112, -0.0252, -0.0289,  ...,  0.0002,  0.0049, -0.0108],\n",
       "          [-0.0004, -0.0207, -0.0224,  ...,  0.0066,  0.0136, -0.0113],\n",
       "          [ 0.0017, -0.0202, -0.0211,  ...,  0.0092,  0.0108, -0.0145],\n",
       "          ...,\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
       "        grad_fn=<TransposeBackward0>),\n",
       " tensor([11, 11, 10, 10, 10,  9,  9,  9,  9,  9,  9,  8,  8,  8,  8,  8,  8,  8,\n",
       "          8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,\n",
       "          8,  8,  8,  8,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,\n",
       "          7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,\n",
       "          7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,\n",
       "          7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  6,  6,  6,\n",
       "          6,  6,  6,  6,  6,  6,  6,  6,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,\n",
       "          5,  5]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_packed_sequence(model.lstm_network(addr_vectors)[0], batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865ebc61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastai",
   "language": "python",
   "name": "fastai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
